{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jeremy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jeremy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.4005180777392556\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text, threshold=2):\n",
    "    months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "\n",
    "    # Instantiate stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(months)\n",
    "    stop_words\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z]+', ' ', text)\n",
    "    # Remove months and people names\n",
    "    \n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove short words\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "\n",
    "    word_counts = Counter(tokens)\n",
    "    filtered_words = [word for word in text.split() if word_counts[word] >= threshold]\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    text = ' '.join(filtered_words)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_optimal_num_topics(articles):\n",
    "    texts = [doc.split() for doc in articles]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    coherence_scores = defaultdict(list)\n",
    "    \n",
    "    for num_topics in range(2, 10):\n",
    "        lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model_lda.get_coherence()\n",
    "        coherence_scores[num_topics].append(coherence_score)\n",
    "\n",
    "    optimal_num_topics = max(coherence_scores, key=lambda k: np.mean(coherence_scores[k]))\n",
    "    return optimal_num_topics\n",
    "\n",
    "\n",
    "def start_lda(preprocessed_content):\n",
    "    # get the optimal number of topics for this date\n",
    "    optimal_num_topics = get_optimal_num_topics(preprocessed_content)\n",
    "\n",
    "    # create the dictionary and corpus\n",
    "    texts = [doc.split() for doc in preprocessed_content]\n",
    "\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # create the LDA model\n",
    "    lda_model = models.LdaModel(corpus, num_topics=optimal_num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "    # calculate the coherence score for the model\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    print(f\"Coherence Score: {coherence_score}\")\n",
    "\n",
    "\n",
    "    vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "    topics = {}\n",
    "    \n",
    "    for i in range(len(vis_data.topic_coordinates)):\n",
    "        topic_num = f\"Topic {i+1}\"\n",
    "        topic_terms = [term for term, _ in lda_model.show_topic(i, topn=12)]\n",
    "        topics[topic_num] = topic_terms\n",
    "\n",
    "        if i == 2:  # stop after getting the top 3 topics\n",
    "            break\n",
    "\n",
    "    with open(\"topics.json\", \"w\") as f:\n",
    "        json.dump(topics, f)\n",
    "    \n",
    "    # topics_df = pd.DataFrame(topics_data.topic_info)\n",
    "    # # get the top three topics based on frequency\n",
    "    # top_topics = topics_df.groupby('Category').size().sort_values(ascending=False).head(3)\n",
    "\n",
    "    # # create a dictionary of the top topics and their terms\n",
    "    # top_topics_dict = {}\n",
    "    # for category in top_topics.index:\n",
    "    #     category_terms = topics_df[topics_df['Category'] == category][['Term', 'Freq']].set_index('Term').to_dict()['Freq']\n",
    "    #     top_topics_dict[category] = category_terms\n",
    "\n",
    "    # # write the dictionary to a JSON file\n",
    "    # with open('top_topics.json', 'w') as f:\n",
    "    #     json.dump(top_topics_dict, f)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def initiate_topic_modelling():\n",
    "    df = pd.read_csv(\"../csv/bmc.csv\", index_col=0)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    # Extract text data from content column and preprocess it\n",
    "    df['preprocessed_content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "    start_lda(df['preprocessed_content'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    initiate_topic_modelling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvisNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "twint 2.1.21 requires aiodns, which is not installed.\n",
      "twint 2.1.21 requires aiohttp, which is not installed.\n",
      "twint 2.1.21 requires aiohttp-socks, which is not installed.\n",
      "twint 2.1.21 requires cchardet, which is not installed.\n",
      "twint 2.1.21 requires elasticsearch, which is not installed.\n",
      "twint 2.1.21 requires fake-useragent, which is not installed.\n",
      "twint 2.1.21 requires geopy, which is not installed.\n",
      "twint 2.1.21 requires googletransx, which is not installed.\n",
      "twint 2.1.21 requires schedule, which is not installed.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 10.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from pyLDAvis) (63.4.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: gensim in d:\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: numexpr in d:\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.3)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting funcy\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting numpy>=1.24.2\n",
      "  Using cached numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Requirement already satisfied: scipy in d:\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.9.1)\n",
      "Collecting pandas>=2.0.0\n",
      "  Using cached pandas-2.0.1-cp39-cp39-win_amd64.whl (10.7 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\jeremy\\appdata\\roaming\\python\\python39\\site-packages (from numexpr->pyLDAvis) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jeremy\\appdata\\roaming\\python\\python39\\site-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n",
      "Installing collected packages: funcy, tzdata, numpy, joblib, pandas, pyLDAvis\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.4\n",
      "    Uninstalling pandas-1.4.4:\n",
      "      Successfully uninstalled pandas-1.4.4\n",
      "Successfully installed funcy-2.0 joblib-1.2.0 numpy-1.24.3 pandas-2.0.1 pyLDAvis-3.4.1 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\anaconda3\\lib\\site-packages (2.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in d:\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
